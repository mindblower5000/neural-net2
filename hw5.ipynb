{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW 5 Мельчук Андрей"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Практическое задание\n",
    "\n",
    "<ol>\n",
    "    <li>Попробуйте изменить параметры нейронной сети работающей с датасетом imdb либо нейронной сети работающей airline-passengers(она прилагается вместе с датасетом к уроку в виде отдельного скрипта) так, чтобы улучшить ее точность. Приложите анализ.</li>\n",
    "    <li>Попробуйте изменить параметры нейронной сети генерирующий текст таким образом, чтобы добиться генерации как можно более осмысленного текста. Пришлите лучший получившейся у вас текст и опишите, что вы предприняли, чтобы его получить. Можно использовать текст другого прозведения.</li>\n",
    "    <li>* Попробуйте на numpy реализовать нейронную сеть архитектуры LSTM</li>\n",
    "    <li>* Предложите свои варианты решения проблемы исчезающего градиента в RNN</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. IMDB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://keras.io/examples/imdb_lstm/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding\n",
    "from keras.layers import LSTM\n",
    "from keras.datasets import imdb\n",
    "\n",
    "max_features = 20000\n",
    "# cut texts after this number of words (among top max_features most common words)\n",
    "maxlen = 80\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "25000 train sequences\n",
      "25000 test sequences\n"
     ]
    }
   ],
   "source": [
    "print('Loading data...')\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "print(len(x_train), 'train sequences')\n",
    "print(len(x_test), 'test sequences')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pad sequences (samples x time)\n",
      "x_train shape: (25000, 80)\n",
      "x_test shape: (25000, 80)\n"
     ]
    }
   ],
   "source": [
    "print('Pad sequences (samples x time)')\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n"
     ]
    }
   ],
   "source": [
    "print('Build model...')\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, 256))\n",
    "model.add(LSTM(256, dropout=0.25, recurrent_dropout=0.25))\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try using different optimizers and different optimizer configs\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/2\n",
      "25000/25000 [==============================] - 264s 11ms/step - loss: 0.4799 - accuracy: 0.7702 - val_loss: 0.4193 - val_accuracy: 0.8224\n",
      "Epoch 2/2\n",
      "25000/25000 [==============================] - 264s 11ms/step - loss: 0.3087 - accuracy: 0.8744 - val_loss: 0.3765 - val_accuracy: 0.8363\n",
      "25000/25000 [==============================] - 62s 2ms/step\n",
      "Test score: 0.3765051734161377\n",
      "Test accuracy: 0.8362799882888794\n"
     ]
    }
   ],
   "source": [
    "print('Train...')\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=2,\n",
    "          validation_data=(x_test, y_test))\n",
    "score, acc = model.evaluate(x_test, y_test,\n",
    "                            batch_size=batch_size)\n",
    "print('Test score:', score)\n",
    "\n",
    "print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    При LTSM/Embedding 128\n",
    "    Test score: 0.4506777255535126\n",
    "    Test accuracy: 0.823360025882721\n",
    "    \n",
    "    Как видно, увеличение количества LTSM блоков до 256 качества сильно не прибавляет, \n",
    "    а время вычисления увеличивает заметно.\n",
    "    Большее число фичей Embedding положительно влияет на качество, но скорость вычисления так же падает.\n",
    "    Увеличение dropout=0.25, recurrent_dropout = 0.25 чуть ухудшило качество по сравнению со значением .2 ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. ALICE IN WONDERLAND"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Итерация #: 0\n",
      "Epoch 1/1\n",
      "158773/158773 [==============================] - 20s 127us/step - loss: 2.4826\n",
      "Генерация из посева: walked off\n",
      "walked off the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the==================================================\n",
      "Итерация #: 1\n",
      "Epoch 1/1\n",
      "158773/158773 [==============================] - 19s 123us/step - loss: 2.0545\n",
      "Генерация из посева: re might b\n",
      "re might be the was in a dont it and the made the pare and the pare and the pare and the pare and the pare and the pare and the pa==================================================\n",
      "Итерация #: 2\n",
      "Epoch 1/1\n",
      "158773/158773 [==============================] - 20s 127us/step - loss: 1.8797\n",
      "Генерация из посева: is, she ca\n",
      "is, she cand the mack the mack the mack the mack the mack the mack the mack the mack the mack the mack the mack the mack the mack ==================================================\n",
      "Итерация #: 3\n",
      "Epoch 1/1\n",
      "158773/158773 [==============================] - 19s 122us/step - loss: 1.7589\n",
      "Генерация из посева: ought to s\n",
      "ought to see what a dont the with the was the could the was the could the was the could the was the could the was the could the wa==================================================\n",
      "Итерация #: 4\n",
      "Epoch 1/1\n",
      "158773/158773 [==============================] - 20s 124us/step - loss: 1.6683\n",
      "Генерация из посева: den. first\n",
      "den. first the sat of the sare and the mork turtle and the mork turtle and the mork turtle and the mork turtle and the mork turtle==================================================\n",
      "Итерация #: 5\n",
      "Epoch 1/1\n",
      "158773/158773 [==============================] - 20s 125us/step - loss: 1.5961\n",
      "Генерация из посева:  they dont\n",
      " they dont and the courted to the poor and the courted to the poor and the courted to the poor and the courted to the poor and the==================================================\n",
      "Итерация #: 6\n",
      "Epoch 1/1\n",
      "158773/158773 [==============================] - 19s 122us/step - loss: 1.5372\n",
      "Генерация из посева:  half an h\n",
      " half an her head to the way all the thing the mouse the thing the mouse the thing the mouse the thing the mouse the thing the mou==================================================\n",
      "Итерация #: 7\n",
      "Epoch 1/1\n",
      "158773/158773 [==============================] - 20s 124us/step - loss: 1.4871\n",
      "Генерация из посева: lice quite\n",
      "lice quite a little she was to her head to herself the mouse the mouse the mouse the mouse the mouse the mouse the mouse the mouse==================================================\n",
      "Итерация #: 8\n",
      "Epoch 1/1\n",
      "158773/158773 [==============================] - 20s 125us/step - loss: 1.4434\n",
      "Генерация из посева: es to proj\n",
      "es to project gutenberg-tm electronic works what it was the mack to the cat the poor and the mock turtle so she see the mock turtl==================================================\n",
      "Итерация #: 9\n",
      "Epoch 1/1\n",
      "158773/158773 [==============================] - 20s 124us/step - loss: 1.4060\n",
      "Генерация из посева: mputer vir\n",
      "mputer viry the project gutenberg-tm electronic works with the way a little best the winder the mock turtle some the mock turtle s==================================================\n",
      "Итерация #: 10\n",
      "Epoch 1/1\n",
      "158773/158773 [==============================] - 20s 124us/step - loss: 1.3717\n",
      "Генерация из посева: ounds unco\n",
      "ounds uncoming to the hatter said to herself the terms of the treally and the trees and the trees and the trees and the trees and ==================================================\n",
      "Итерация #: 11\n",
      "Epoch 1/1\n",
      "158773/158773 [==============================] - 20s 124us/step - loss: 1.3418\n",
      "Генерация из посева: ay, do cat\n",
      "ay, do cat the project gutenberg-tm electronic works when the rest of the morse as she was good nerthing to the project gutenberg-==================================================\n",
      "Итерация #: 12\n",
      "Epoch 1/1\n",
      "158773/158773 [==============================] - 20s 125us/step - loss: 1.3142\n",
      "Генерация из посева: en! the ki\n",
      "en! the king said to herself in a little birds the mock turtle some of the words as she was a little birds the mock turtle some of==================================================\n",
      "Итерация #: 13\n",
      "Epoch 1/1\n",
      "158773/158773 [==============================] - 20s 128us/step - loss: 1.2886\n",
      "Генерация из посева: d to think\n",
      "d to think that in a moment to be a more to be a more to be a more to be a more to be a more to be a more to be a more to be a mor==================================================\n",
      "Итерация #: 14\n",
      "Epoch 1/1\n",
      "158773/158773 [==============================] - 20s 124us/step - loss: 1.2660\n",
      "Генерация из посева: it exists \n",
      "it exists and distributing and she had not as she was a little best had been that it was a grown and the same to see in a long and==================================================\n",
      "Итерация #: 15\n",
      "Epoch 1/1\n",
      "158773/158773 [==============================] - 20s 126us/step - loss: 1.2450\n",
      "Генерация из посева: they both \n",
      "they both a can of the sormagain one of the sormagain one of the sormagain one of the sormagain one of the sormagain one of the so==================================================\n",
      "Итерация #: 16\n",
      "Epoch 1/1\n",
      "158773/158773 [==============================] - 20s 128us/step - loss: 1.2243\n",
      "Генерация из посева:  right hou\n",
      " right houdes the mock turtle she was not a really said the mock turtle she was not a really said the mock turtle she was not a re==================================================\n",
      "Итерация #: 17\n",
      "Epoch 1/1\n",
      "158773/158773 [==============================] - 20s 123us/step - loss: 1.2059\n",
      "Генерация из посева: ce, whose \n",
      "ce, whose the rest of the sendence the trees had to get in the seated to her feet of the court, and the morme had been the trees h==================================================\n",
      "Итерация #: 18\n",
      "Epoch 1/1\n",
      "158773/158773 [==============================] - 20s 125us/step - loss: 1.1880\n",
      "Генерация из посева: nd there s\n",
      "nd there said to herself, and the mock turtle said the mock turtle said the mock turtle said the mock turtle said the mock turtle ==================================================\n",
      "Итерация #: 19\n",
      "Epoch 1/1\n",
      "158773/158773 [==============================] - 20s 125us/step - loss: 1.1715\n",
      "Генерация из посева:  rabbit re\n",
      " rabbit reading to the project gutenberg-tm electronic works when it was the door a distributing and before she had been was going==================================================\n",
      "Итерация #: 20\n",
      "Epoch 1/1\n",
      "158773/158773 [==============================] - 20s 125us/step - loss: 1.1549\n",
      "Генерация из посева: wnwards, a\n",
      "wnwards, and the mock turtle said to herself in a low the project gutenberg-tm work in a long as the same things a down the king, ==================================================\n",
      "Итерация #: 21\n",
      "Epoch 1/1\n",
      "158773/158773 [==============================] - 20s 124us/step - loss: 1.1394\n",
      "Генерация из посева: nd condemn\n",
      "nd condemnty a really said the duchess was a treacle said the duchess was a treacle said the duchess was a treacle said the duches==================================================\n",
      "Итерация #: 22\n",
      "Epoch 1/1\n",
      "158773/158773 [==============================] - 20s 125us/step - loss: 1.1246\n",
      "Генерация из посева: hat a deli\n",
      "hat a delight in the seagle a little shriek of the words as she was not a real to the project gutenberg literary archive foundatio==================================================\n",
      "Итерация #: 23\n",
      "Epoch 1/1\n",
      "158773/158773 [==============================] - 20s 124us/step - loss: 1.1103\n",
      "Генерация из посева:  that the \n",
      " that the mouse donations to the project gutenberg-tm electronic works in the seated to her such a crasing, and she thought to her==================================================\n",
      "Итерация #: 24\n",
      "Epoch 1/1\n",
      "158773/158773 [==============================] - 20s 126us/step - loss: 1.0963\n",
      "Генерация из посева:  brown hai\n",
      " brown hair the mock turtle said to herself, and she was now and the particular spread of the mock turtle said to herself, and she==================================================\n",
      "Итерация #: 25\n",
      "Epoch 1/1\n",
      "158773/158773 [==============================] - 20s 125us/step - loss: 1.0825\n",
      "Генерация из посева: , tossing \n",
      ", tossing the project gutenberg-tm electronic works what they were all that the mouse of the trees had too make one that she was n==================================================\n",
      "Итерация #: 26\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "158773/158773 [==============================] - 20s 128us/step - loss: 1.0699\n",
      "Генерация из посева: like, but \n",
      "like, but the mouse was the mouse was the mouse was the mouse was the mouse was the mouse was the mouse was the mouse was the mous==================================================\n",
      "Итерация #: 27\n",
      "Epoch 1/1\n",
      "158773/158773 [==============================] - 20s 123us/step - loss: 1.0583\n",
      "Генерация из посева: hat they c\n",
      "hat they cant her hands of course it would be no doesnt go on with the duchess was a little sharply and the poor little thing is a==================================================\n",
      "Итерация #: 28\n",
      "Epoch 1/1\n",
      "158773/158773 [==============================] - 19s 123us/step - loss: 1.0451\n",
      "Генерация из посева: ern, with \n",
      "ern, with a baby with its eyes, and the dormouse shook his the best cater if it was the door of the project gutenberg-tm electroni==================================================\n",
      "Итерация #: 29\n",
      "Epoch 1/1\n",
      "158773/158773 [==============================] - 20s 126us/step - loss: 1.0333\n",
      "Генерация из посева:  see me th\n",
      " see me the mouse that it made out of the work as it was a little wish the court, and then the project gutenberg-tm electronic wor==================================================\n",
      "Итерация #: 30\n",
      "Epoch 1/1\n",
      "158773/158773 [==============================] - 20s 124us/step - loss: 1.0227\n",
      "Генерация из посева: t? she tho\n",
      "t? she thought that she had not a sor of the full project gutenberg-tm electronic works in the wind what would be offended tone, w==================================================\n",
      "Итерация #: 31\n",
      "Epoch 1/1\n",
      "158773/158773 [==============================] - 20s 125us/step - loss: 1.0115\n",
      "Генерация из посева: ther child\n",
      "ther children what i wont the caterpillar. alice thought the permoss when the rest of the sea-- [i havent soup of the sulded to he==================================================\n",
      "Итерация #: 32\n",
      "Epoch 1/1\n",
      "158773/158773 [==============================] - 20s 125us/step - loss: 1.0011\n",
      "Генерация из посева:  in a tone\n",
      " in a tone of the soldiers was a little best the stupide to the dormouse sounded to the project gutenberg-tm works that is such a ==================================================\n",
      "Итерация #: 33\n",
      "Epoch 1/1\n",
      "158773/158773 [==============================] - 20s 128us/step - loss: 0.9908\n",
      "Генерация из посева:  really dr\n",
      " really dreadfully took the confication at all remember the browe after the other birds to feng in the state of the trees had a lo==================================================\n",
      "Итерация #: 34\n",
      "Epoch 1/1\n",
      "158773/158773 [==============================] - 21s 130us/step - loss: 0.9792\n",
      "Генерация из посева: shared the\n",
      "shared the queen said to the project gutenberg-tm electronic work is down at the door a poor alice, and she said to herself that s==================================================\n",
      "Итерация #: 35\n",
      "Epoch 1/1\n",
      "158773/158773 [==============================] - 20s 127us/step - loss: 0.9703\n",
      "Генерация из посева: ss office \n",
      "ss office in her life to herself, i dont know what to be a minate or two she heard a little shart of the top of its head to over t==================================================\n",
      "Итерация #: 36\n",
      "Epoch 1/1\n",
      "158773/158773 [==============================] - 20s 125us/step - loss: 0.9601\n",
      "Генерация из посева: ke that! s\n",
      "ke that! said the king said, and the white rabbit was a little best then it was the white rabbit was a little best then it was the==================================================\n",
      "Итерация #: 37\n",
      "Epoch 1/1\n",
      "158773/158773 [==============================] - 19s 122us/step - loss: 0.9511\n",
      "Генерация из посева: ry curious\n",
      "ry curious to the caterpillar and the browe, she said to herself, and the end of the canchth out itself the right of replacement o==================================================\n",
      "Итерация #: 38\n",
      "Epoch 1/1\n",
      "158773/158773 [==============================] - 19s 122us/step - loss: 0.9423\n",
      "Генерация из посева: e part of \n",
      "e part of this little an of any other work as if it had gone up into the mouse only a preacred the gryphon went on, the mock turtl==================================================\n",
      "Итерация #: 39\n",
      "Epoch 1/1\n",
      "158773/158773 [==============================] - 20s 123us/step - loss: 0.9335\n",
      "Генерация из посева: the hedgeh\n",
      "the hedgehog was sitting on the suppressed very never went down and a large cat up again, and the mock turtle said to herself, and==================================================\n",
      "Итерация #: 40\n",
      "Epoch 1/1\n",
      "158773/158773 [==============================] - 19s 123us/step - loss: 0.9253\n",
      "Генерация из посева:  riddles t\n",
      " riddles the door of the table to herself, and she thought that she was to the caterpillar. alice thought that she was to the cate==================================================\n",
      "Итерация #: 41\n",
      "Epoch 1/1\n",
      "158773/158773 [==============================] - 20s 125us/step - loss: 0.9168\n",
      "Генерация из посева: peak--and \n",
      "peak--and the baby with one finger to alice, as she said to herself, and the mock turtle. well, i should like to see it again, so ==================================================\n",
      "Итерация #: 42\n",
      "Epoch 1/1\n",
      "158773/158773 [==============================] - 21s 132us/step - loss: 0.9090\n",
      "Генерация из посева: illiam the\n",
      "illiam the march hare said to herself, and the mock turtle said to herself, and the mock turtle said to herself, and the mock turt==================================================\n",
      "Итерация #: 43\n",
      "Epoch 1/1\n",
      "158773/158773 [==============================] - 20s 127us/step - loss: 0.9005\n",
      "Генерация из посева: d fellow! \n",
      "d fellow! said the dormouse say which was that it might have the proper way of courte--any hasen in the soldiers was the white rab==================================================\n",
      "Итерация #: 44\n",
      "Epoch 1/1\n",
      "158773/158773 [==============================] - 20s 128us/step - loss: 0.8935\n",
      "Генерация из посева: u must obt\n",
      "u must obtain permission of the court, and they were all the terms of this agreement any project gutenberg-tm works and she went o==================================================\n",
      "Итерация #: 45\n",
      "Epoch 1/1\n",
      "158773/158773 [==============================] - 21s 130us/step - loss: 0.8863\n",
      "Генерация из посева:  effect of\n",
      " effect of license included alice, as she said to herself, and she said to herself, and she said to herself, and she said to herse==================================================\n",
      "Итерация #: 46\n",
      "Epoch 1/1\n",
      "158773/158773 [==============================] - 20s 127us/step - loss: 0.8784\n",
      "Генерация из посева:  that it w\n",
      " that it was a little wish they were like came indeed! said the mock turtle said to herself, and she said to herself, and she said==================================================\n",
      "Итерация #: 47\n",
      "Epoch 1/1\n",
      "158773/158773 [==============================] - 20s 127us/step - loss: 0.8721\n",
      "Генерация из посева: e shall i \n",
      "e shall i wonder what i should be rendint the project gutenberg-tm works and seemed to be suce did had never peesing of the court,==================================================\n",
      "Итерация #: 48\n",
      "Epoch 1/1\n",
      "158773/158773 [==============================] - 21s 131us/step - loss: 0.8656\n",
      "Генерация из посева: lly, the p\n",
      "lly, the patering tone. alice looked at alice as he spoke. and then the reason in the soldiers was the shate as she could not to g==================================================\n",
      "Итерация #: 49\n",
      "Epoch 1/1\n",
      "158773/158773 [==============================] - 21s 130us/step - loss: 0.8582\n",
      "Генерация из посева: nd, watchi\n",
      "nd, watching the conqueror, (for some time with the dormouse said to herself, and she went on and followed a large caterpillar sor==================================================\n",
      "Итерация #: 50\n",
      "Epoch 1/1\n",
      "158773/158773 [==============================] - 21s 130us/step - loss: 0.8514\n",
      "Генерация из посева: e owl had \n",
      "e owl had the next witness was the white rabbit, who was passing at the court, and and she heard a little sharp of the teacups as ==================================================\n",
      "Итерация #: 51\n",
      "Epoch 1/1\n",
      "158773/158773 [==============================] - 20s 127us/step - loss: 0.8462\n",
      "Генерация из посева: hey may be\n",
      "hey may be one. who had spechess work iss the chimneys, or hear it was going to say that it might have the proped with the little ==================================================\n",
      "Итерация #: 52\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "158773/158773 [==============================] - 20s 128us/step - loss: 0.8400\n",
      "Генерация из посева: r tongue, \n",
      "r tongue, said alice, and she went on again, alice thought the poor his head out the round she heard a little way off, and began s==================================================\n",
      "Итерация #: 53\n",
      "Epoch 1/1\n",
      "158773/158773 [==============================] - 20s 126us/step - loss: 0.8338\n",
      "Генерация из посева: t.  if you\n",
      "t.  if you dont let me supld as she could not off, said the caterpillar took the house, the mock turtle, said the caterpillar took==================================================\n",
      "Итерация #: 54\n",
      "Epoch 1/1\n",
      "158773/158773 [==============================] - 20s 128us/step - loss: 0.8270\n",
      "Генерация из посева: ell, if i \n",
      "ell, if i no roke. then herepted to see if the popide a finis ebooks, and she dread behowing that she said to herself, and off wit==================================================\n",
      "Итерация #: 55\n",
      "Epoch 1/1\n",
      "158773/158773 [==============================] - 20s 126us/step - loss: 0.8219\n",
      "Генерация из посева:  and the m\n",
      " and the moral of that is any distributing or project gutenberg-tm works and had no does and began singing in the seaside the quee==================================================\n",
      "Итерация #: 56\n",
      "Epoch 1/1\n",
      "158773/158773 [==============================] - 20s 126us/step - loss: 0.8153\n",
      "Генерация из посева: dvice from\n",
      "dvice from a great letter, they say then it was a little wish they were getting so fan and shouted out a project gutenberg-tm elec==================================================\n",
      "Итерация #: 57\n",
      "Epoch 1/1\n",
      "158773/158773 [==============================] - 20s 129us/step - loss: 0.8110\n",
      "Генерация из посева: w im mad? \n",
      "w im mad? said alice. im good-- if it makes downtudes in silence. alice felt theres and the baby with the beginning to be the rest==================================================\n",
      "Итерация #: 58\n",
      "Epoch 1/1\n",
      "158773/158773 [==============================] - 21s 130us/step - loss: 0.8049\n",
      "Генерация из посева: : the marc\n",
      ": the march hare interrupted the house of the project gutenberg-tm license must make may with project gutenberg-tm license must ma==================================================\n",
      "Итерация #: 59\n",
      "Epoch 1/1\n",
      "158773/158773 [==============================] - 21s 131us/step - loss: 0.7998\n",
      "Генерация из посева:  and she d\n",
      " and she doft to on any one of the other side of the court, are alice opened the roof of the house of the court, are alice opened ==================================================\n",
      "Итерация #: 60\n",
      "Epoch 1/1\n",
      "158773/158773 [==============================] - 20s 127us/step - loss: 0.7941\n",
      "Генерация из посева: t is! as s\n",
      "t is! as she said to herself, and she said to herself, and she said to herself, and she said to herself, and she said to herself, ==================================================\n",
      "Итерация #: 61\n",
      "Epoch 1/1\n",
      "158773/158773 [==============================] - 20s 127us/step - loss: 0.7896\n",
      "Генерация из посева: th its win\n",
      "th its winnen as she was a little birds and went on the state of change of the project gutenberg-tm work, and was going off at onc==================================================\n",
      "Итерация #: 62\n",
      "Epoch 1/1\n",
      "158773/158773 [==============================] - 21s 129us/step - loss: 0.7853\n",
      "Генерация из посева: ming. it w\n",
      "ming. it was a little sharp firitis violenter to copy if i will she did not like to ground to the project gutenberg-tm work (any t==================================================\n",
      "Итерация #: 63\n",
      "Epoch 1/1\n",
      "158773/158773 [==============================] - 20s 125us/step - loss: 0.7805\n",
      "Генерация из посева: ake person\n",
      "ake personal remarks going there was no one ought to be ashaps cried how come back in a hurry: a large play to so shoald alice as ==================================================\n",
      "Итерация #: 64\n",
      "Epoch 1/1\n",
      "158773/158773 [==============================] - 20s 128us/step - loss: 0.7764\n",
      "Генерация из посева: s arms, to\n",
      "s arms, took the house, and which was to the other side of the ground as found it advisable to alice, the duchess; and that it was==================================================\n",
      "Итерация #: 65\n",
      "Epoch 1/1\n",
      "158773/158773 [==============================] - 21s 130us/step - loss: 0.7716\n",
      "Генерация из посева:  people in\n",
      " people in a smin in hanting alice. to say in the middle, alice could only say through the door, she was as she jumped up and walk==================================================\n",
      "Итерация #: 66\n",
      "Epoch 1/1\n",
      "158773/158773 [==============================] - 20s 126us/step - loss: 0.7670\n",
      "Генерация из посева:  the night\n",
      " the night at the sea--out! so i slook work with the dormouse shook its hearthout as she was sure to have handed to way, to go on ==================================================\n",
      "Итерация #: 67\n",
      "Epoch 1/1\n",
      "158773/158773 [==============================] - 20s 128us/step - loss: 0.7622\n",
      "Генерация из посева: , she do. \n",
      ", she do. it was as she jumped up and walking at the court, and they went on in a louder tone, and the moral of that is ang moke o==================================================\n",
      "Итерация #: 68\n",
      "Epoch 1/1\n",
      "158773/158773 [==============================] - 20s 125us/step - loss: 0.7587\n",
      "Генерация из посева: , swallowi\n",
      ", swallowing down in a found on who you are old, said the duchess to play croquet. the freg ow somebleabered a brinking the doorar==================================================\n",
      "Итерация #: 69\n",
      "Epoch 1/1\n",
      "158773/158773 [==============================] - 20s 127us/step - loss: 0.7539\n",
      "Генерация из посева: en it had \n",
      "en it had for her to spil i ve coundrale, and the party were this moment the first thing i heard as my diffored and did not answer==================================================\n",
      "Итерация #: 70\n",
      "Epoch 1/1\n",
      "158773/158773 [==============================] - 20s 127us/step - loss: 0.7518\n",
      "Генерация из посева: ok, rule f\n",
      "ok, rule fortitinging three were the verses the great wonder is to the king, that it was in a sorrowful tone, and the poor little ==================================================\n",
      "Итерация #: 71\n",
      "Epoch 1/1\n",
      "158773/158773 [==============================] - 20s 128us/step - loss: 0.7466\n",
      "Генерация из посева:  old woman\n",
      " old woman encouthous creature at encoush out of shouted the project gutenberg-tm concept of a large as her an angriesed that down==================================================\n",
      "Итерация #: 72\n",
      "Epoch 1/1\n",
      "158773/158773 [==============================] - 20s 126us/step - loss: 0.7426\n",
      "Генерация из посева:  see anyth\n",
      " see anything, said alice, who felt ready to take the right to our sole the garden, and said to the king, the queen, and she tried==================================================\n",
      "Итерация #: 73\n",
      "Epoch 1/1\n",
      "158773/158773 [==============================] - 20s 129us/step - loss: 0.7388\n",
      "Генерация из посева: opeless th\n",
      "opeless than a fire, and the moral of that is any distume outhed timidly, and letcuded to a mose of the month is it his garden, an==================================================\n",
      "Итерация #: 74\n",
      "Epoch 1/1\n",
      "158773/158773 [==============================] - 20s 125us/step - loss: 0.7364\n",
      "Генерация из посева: them off, \n",
      "them off, said the duchess said to alice, and say it a remover! so she went on eagerly, theres the footman, and alice was some of ==================================================\n",
      "Итерация #: 75\n",
      "Epoch 1/1\n",
      "158773/158773 [==============================] - 20s 124us/step - loss: 0.7319\n",
      "Генерация из посева: n guess th\n",
      "n guess that had some time with one finger very deeply folled to be to come befount along the tea-pittly used to do you think youd==================================================\n",
      "Итерация #: 76\n",
      "Epoch 1/1\n",
      "158773/158773 [==============================] - 20s 126us/step - loss: 0.7285\n",
      "Генерация из посева:  tax exemp\n",
      " tax exempt status in a low voice, what some wine, the mouse was sitting on the same thing i heard impatiently, and said that all ==================================================\n",
      "Итерация #: 77\n",
      "Epoch 1/1\n",
      "158773/158773 [==============================] - 20s 129us/step - loss: 0.7255\n",
      "Генерация из посева: ad been al\n",
      "ad been all to herrelt, but she remembered that she had not a some le: nours belted editions will be a befor and sent, who was to ==================================================\n",
      "Итерация #: 78\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "158773/158773 [==============================] - 20s 124us/step - loss: 0.7209\n",
      "Генерация из посева: n i learn \n",
      "n i learn of the tea-pittlen alise--bole, and she starps cook to knew the mouse to the conchuprantle she he one, said alice, and s==================================================\n",
      "Итерация #: 79\n",
      "Epoch 1/1\n",
      "158773/158773 [==============================] - 20s 127us/step - loss: 0.7173\n",
      "Генерация из посева: t to bring\n",
      "t to bring peet in a far out to the little golden key was the duchess said again! said alice, as she was a little bitllente a shro==================================================\n",
      "Итерация #: 80\n",
      "Epoch 1/1\n",
      "158773/158773 [==============================] - 20s 124us/step - loss: 0.7153\n",
      "Генерация из посева:  with that\n",
      " with that she was now am not a good deal for enting trisled down the hatter was the white rabbit, who seemed to be nothing more t==================================================\n",
      "Итерация #: 81\n",
      "Epoch 1/1\n",
      "158773/158773 [==============================] - 20s 125us/step - loss: 0.7119\n",
      "Генерация из посева: his moment\n",
      "his moment the fies at help itself, and said to the jury. the rabbit came near her, and the moral of that is--the white rabbit rea==================================================\n",
      "Итерация #: 82\n",
      "Epoch 1/1\n",
      "158773/158773 [==============================] - 19s 122us/step - loss: 0.7077\n",
      "Генерация из посева: oarsely al\n",
      "oarsely along the beal of the treatree the equenting do neat it would be of added up back too plection to preted in the other bird==================================================\n",
      "Итерация #: 83\n",
      "Epoch 1/1\n",
      "158773/158773 [==============================] - 20s 123us/step - loss: 0.7067\n",
      "Генерация из посева: e what the\n",
      "e what the next witness was compotted is agoud this kidn, and of not providicgaregig lat eatres in the sea. the mouse was spread m==================================================\n",
      "Итерация #: 84\n",
      "Epoch 1/1\n",
      "158773/158773 [==============================] - 20s 123us/step - loss: 0.7033\n",
      "Генерация из посева:  came upon\n",
      " came upon a little bit alice, said the mock turtle said: i didnt braks dight by the extraimented as he cound, into all this time ==================================================\n",
      "Итерация #: 85\n",
      "Epoch 1/1\n",
      "158773/158773 [==============================] - 20s 126us/step - loss: 0.6997\n",
      "Генерация из посева:  terms of \n",
      " terms of the full extent permitted by the white rabbit reed under the time he say there was a very tride a winn or recourally int==================================================\n",
      "Итерация #: 86\n",
      "Epoch 1/1\n",
      "158773/158773 [==============================] - 20s 125us/step - loss: 0.6974\n",
      "Генерация из посева: first figu\n",
      "first figure, said the duchess said and it, said the duchess said and it, said the duchess said and it, said the duchess said and ==================================================\n",
      "Итерация #: 87\n",
      "Epoch 1/1\n",
      "158773/158773 [==============================] - 20s 128us/step - loss: 0.6947\n",
      "Генерация из посева: --a cheshi\n",
      "--a cheshire cats and her arcouse, and when she got used to it an eastanchely, alice replied in an offended tone, were on it in li==================================================\n",
      "Итерация #: 88\n",
      "Epoch 1/1\n",
      "158773/158773 [==============================] - 20s 127us/step - loss: 0.6922\n",
      "Генерация из посева: he could h\n",
      "he could hear the rabbit say things to her ordered in a smines arm conely to find among the trees under her arms folded the pature==================================================\n",
      "Итерация #: 89\n",
      "Epoch 1/1\n",
      "158773/158773 [==============================] - 20s 126us/step - loss: 0.6898\n",
      "Генерация из посева: not be rea\n",
      "not be read by it it in all 50 states of the shesh rate they seem to be gain as any minutes from of project gutenberg-tm electroni==================================================\n",
      "Итерация #: 90\n",
      "Epoch 1/1\n",
      "158773/158773 [==============================] - 20s 127us/step - loss: 0.6857\n",
      "Генерация из посева: custody an\n",
      "custody and say with an out of the work on sat beto that? if under helpers she was surtrise tone. no, ill be us! said the caterpil==================================================\n",
      "Итерация #: 91\n",
      "Epoch 1/1\n",
      "158773/158773 [==============================] - 20s 127us/step - loss: 0.6842\n",
      "Генерация из посева: hardly kno\n",
      "hardly knownthing the queen of hearts, who out of the work in any particular pans makes for the white rabbit, with a send it shatc==================================================\n",
      "Итерация #: 92\n",
      "Epoch 1/1\n",
      "158773/158773 [==============================] - 20s 127us/step - loss: 0.6805\n",
      "Генерация из посева: whether it\n",
      "whether it was the white kid gloves in the pictured the mouse, who was passing of compliance with payag, preading moment it was ag==================================================\n",
      "Итерация #: 93\n",
      "Epoch 1/1\n",
      "158773/158773 [==============================] - 20s 125us/step - loss: 0.6793\n",
      "Генерация из посева: any projec\n",
      "any project gutenberg-tm collection are you cant help it, said the mock turtle say they words: she asked, we curious to donate roy==================================================\n",
      "Итерация #: 94\n",
      "Epoch 1/1\n",
      "158773/158773 [==============================] - 21s 130us/step - loss: 0.6758\n",
      "Генерация из посева: ce some ot\n",
      "ce some other surcionl was it is? i didnt beautiful soup! chapter xii. and when she had forgotten the duchess, chorks his great ey==================================================\n",
      "Итерация #: 95\n",
      "Epoch 1/1\n",
      "158773/158773 [==============================] - 20s 127us/step - loss: 0.6742\n",
      "Генерация из посева: mber, rema\n",
      "mber, remarked they all she could not tell what are your song, and the moral of that is--be wanten or forgutted eeliced away would==================================================\n",
      "Итерация #: 96\n",
      "Epoch 1/1\n",
      "158773/158773 [==============================] - 20s 123us/step - loss: 0.6718\n",
      "Генерация из посева: asing the \n",
      "asing the stull now, oh, said the caterpillar was the first to begin we was such a harried by the patuce to presect, which sones t==================================================\n",
      "Итерация #: 97\n",
      "Epoch 1/1\n",
      "158773/158773 [==============================] - 20s 124us/step - loss: 0.6694\n",
      "Генерация из посева: f out agai\n",
      "f out again. dont go on with the terms of this agreement shall not stiep the sen subjeck, tou with use know--and then alice dodok,==================================================\n",
      "Итерация #: 98\n",
      "Epoch 1/1\n",
      "158773/158773 [==============================] - 20s 126us/step - loss: 0.6678\n",
      "Генерация из посева: mply with \n",
      "mply with a soldier in severand it size? why, i went to the project gutenberg-tm license included with the lorges at the door a so==================================================\n",
      "Итерация #: 99\n",
      "Epoch 1/1\n",
      "158773/158773 [==============================] - 20s 126us/step - loss: 0.6656\n",
      "Генерация из посева:  personal \n",
      " personal remarks beater-- you couldnt tell what a dited the gryphon replied rather anxiously at the duchess, and the little pitte\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.layers.recurrent import SimpleRNN, LSTM, GRU\n",
    "from keras.models import Sequential\n",
    "from keras.regularizers import l1\n",
    "\n",
    "\n",
    "# построчное чтение из примера с текстом \n",
    "with open(\"alice_in_wonderland.txt\", 'rb') as _in:\n",
    "    lines = []\n",
    "    for line in _in:\n",
    "        line = line.strip().lower().decode(\"ascii\", \"ignore\")\n",
    "        if len(line) == 0:\n",
    "            continue\n",
    "        lines.append(line)\n",
    "text = \" \".join(lines)\n",
    "chars = set([c for c in text])\n",
    "nb_chars = len(chars)\n",
    "\n",
    "\n",
    "# создание индекса символов и reverse mapping чтобы передвигаться между значениями numerical\n",
    "# ID and a specific character. The numerical ID will correspond to a column\n",
    "# ID и определенный символ. Numerical ID будет соответсвовать колонке\n",
    "# число при использовании one-hot кодировки для представление входов символов\n",
    "char2index = {c: i for i, c in enumerate(chars)}\n",
    "index2char = {i: c for i, c in enumerate(chars)}\n",
    "\n",
    "# для удобства выберете фиксированную длину последовательность 10 символов \n",
    "SEQLEN, STEP = 10, 1\n",
    "input_chars, label_chars = [], []\n",
    "\n",
    "# конвертация data в серии разных SEQLEN-length субпоследовательностей\n",
    "for i in range(0, len(text) - SEQLEN, STEP):\n",
    "    input_chars.append(text[i: i + SEQLEN])\n",
    "    label_chars.append(text[i + SEQLEN])\n",
    "\n",
    "\n",
    "# Вычисление one-hot encoding входных последовательностей X и следующего символа (the label) y\n",
    "\n",
    "X = np.zeros((len(input_chars), SEQLEN, nb_chars), dtype=np.bool)\n",
    "y = np.zeros((len(input_chars), nb_chars), dtype=np.bool)\n",
    "for i, input_char in enumerate(input_chars):\n",
    "    for j, ch in enumerate(input_char):\n",
    "        X[i, j, char2index[ch]] = 1\n",
    "    y[i, char2index[label_chars[i]]] = 1\n",
    "\n",
    "\n",
    "# установка ряда метапамертров  для нейронной сети и процесса тренировки\n",
    "BATCH_SIZE, HIDDEN_SIZE = 128, 128\n",
    "NUM_ITERATIONS = 100 # 25 должно быть достаточно\n",
    "NUM_EPOCHS_PER_ITERATION = 1\n",
    "NUM_PREDS_PER_EPOCH = 120\n",
    "\n",
    "\n",
    "# Create a super simple recurrent neural network. There is one recurrent\n",
    "# layer that produces an embedding of size HIDDEN_SIZE from the one-hot\n",
    "# encoded input layer. This is followed by a Dense fully-connected layer\n",
    "# across the set of possible next characters, which is converted to a\n",
    "# probability score via a standard softmax activation with a multi-class\n",
    "# cross-entropy loss function linking the prediction to the one-hot\n",
    "# encoding character label.\n",
    "\n",
    "'''\n",
    "Создание очень простой рекуррентной нейронной сети. В ней будет один реккурентный закодированный входной слой. За ним последует полносвязный слой связанный с набором возможных следующих символов, которые конвертированы в вероятностные результаты через стандартную softmax активацию с multi-class cross-encoding loss функцию ссылающуются на предсказание one-hot encoding лейбл символа\n",
    "'''\n",
    "\n",
    "model = Sequential()\n",
    "model.add(\n",
    "    LSTM(  # вы можете изменить эту часть на LSTM или SimpleRNN, чтобы попробовать альтернативы\n",
    "        HIDDEN_SIZE,\n",
    "        return_sequences=False,\n",
    "        input_shape=(SEQLEN, nb_chars),\n",
    "#         dropout=0.05,\n",
    "#         activity_regularizer=l1(0.001),\n",
    "        unroll=True\n",
    "    )\n",
    ")\n",
    "model.add(Dense(nb_chars))\n",
    "model.add(Activation(\"softmax\"))\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\") #rmsprop\n",
    "\n",
    "\n",
    "# выполнение серий тренировочных и демонстрационных итераций \n",
    "for iteration in range(NUM_ITERATIONS):\n",
    "\n",
    "    # для каждой итерации запуск передачи данных в модель \n",
    "    print(\"=\" * 50)\n",
    "    print(\"Итерация #: %d\" % (iteration))\n",
    "    model.fit(X, y, batch_size=BATCH_SIZE, epochs=NUM_EPOCHS_PER_ITERATION)\n",
    "\n",
    "    # Select a random example input sequence.\n",
    "    test_idx = np.random.randint(len(input_chars))\n",
    "    test_chars = input_chars[test_idx]\n",
    "\n",
    "    # для числа шагов предсказаний использование текущей тренируемой модели \n",
    "    # конструирование one-hot encoding для тестирования input и добавление предсказания.\n",
    "    print(\"Генерация из посева: %s\" % (test_chars))\n",
    "    print(test_chars, end=\"\")\n",
    "    for i in range(NUM_PREDS_PER_EPOCH):\n",
    "\n",
    "        # здесь one-hot encoding.\n",
    "        X_test = np.zeros((1, SEQLEN, nb_chars))\n",
    "        for j, ch in enumerate(test_chars):\n",
    "            X_test[0, j, char2index[ch]] = 1\n",
    "\n",
    "        # осуществление предсказания с помощью текущей модели.\n",
    "        pred = model.predict(X_test, verbose=0)[0]\n",
    "        y_pred = index2char[np.argmax(pred)]\n",
    "\n",
    "        # вывод предсказания добавленного к тестовому примеру \n",
    "        print(y_pred, end=\"\")\n",
    "\n",
    "        # инкрементация тестового примера содержащего предсказание\n",
    "        test_chars = test_chars[1:] + y_pred\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Результат\n",
    "\n",
    "В итоге лучшее предложение,\n",
    "\n",
    "\"personal remarks beater-- you couldnt tell what a dited the gryphon replied rather anxiously at the duchess, and the little pitte\"\n",
    "\n",
    "\"личные реплики, ты не мог бы сказать, что грифон довольно беспокойно ответил на герцогиню и маленькую питте\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Увеличениие BATCH_SIZE, HIDDEN_SIZE не помогло. А вот большее число итераций улучшает результат и позволяет стоить все более хитрые предложения."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. LSTM on numpy\n",
    "\n",
    "Как реализовано, изучил отсюда. \n",
    "\n",
    "https://blog.varunajayasiri.com/numpy_lstm.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сеть обучается с использованием стохастического градиентного спуска с использованием алгоритма AdaGrad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "plt.style.use('seaborn-white')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = open(\"alice_in_wonderland.txt\", 'r').read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data has 163815 characters, 86 unique\n"
     ]
    }
   ],
   "source": [
    "chars = list(set(data))\n",
    "data_size, X_size = len(data), len(chars)\n",
    "print(\"data has %d characters, %d unique\" % (data_size, X_size))\n",
    "char_to_idx = {ch:i for i,ch in enumerate(chars)}\n",
    "idx_to_char = {i:ch for i,ch in enumerate(chars)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "H_size = 100 # Size of the hidden layer\n",
    "T_steps = 25 # Number of time steps (length of the sequence) used for training\n",
    "learning_rate = 1e-1 # Learning rate\n",
    "weight_sd = 0.1 # Standard deviation of weights for initialization\n",
    "z_size = H_size + X_size # Size of concatenate(H, X) vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "def dsigmoid(y):\n",
    "    return y * (1 - y)\n",
    "\n",
    "\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "\n",
    "def dtanh(y):\n",
    "    return 1 - y * y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Param:\n",
    "    def __init__(self, name, value):\n",
    "        self.name = name\n",
    "        self.v = value #parameter value\n",
    "        self.d = np.zeros_like(value) #derivative\n",
    "        self.m = np.zeros_like(value) #momentum for AdaGrad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Parameters:    \n",
    "    def __init__(self):\n",
    "        self.W_f = Param('W_f', \n",
    "                         np.random.randn(H_size, z_size) * weight_sd + 0.5)\n",
    "        self.b_f = Param('b_f',\n",
    "                         np.zeros((H_size, 1)))\n",
    "\n",
    "        self.W_i = Param('W_i',\n",
    "                         np.random.randn(H_size, z_size) * weight_sd + 0.5)\n",
    "        self.b_i = Param('b_i',\n",
    "                         np.zeros((H_size, 1)))\n",
    "\n",
    "        self.W_C = Param('W_C',\n",
    "                         np.random.randn(H_size, z_size) * weight_sd)\n",
    "        self.b_C = Param('b_C',\n",
    "                         np.zeros((H_size, 1)))\n",
    "\n",
    "        self.W_o = Param('W_o',\n",
    "                         np.random.randn(H_size, z_size) * weight_sd + 0.5)\n",
    "        self.b_o = Param('b_o',\n",
    "                         np.zeros((H_size, 1)))\n",
    "\n",
    "        #For final layer to predict the next character\n",
    "        self.W_v = Param('W_v',\n",
    "                         np.random.randn(X_size, H_size) * weight_sd)\n",
    "        self.b_v = Param('b_v',\n",
    "                         np.zeros((X_size, 1)))\n",
    "        \n",
    "    def all(self):\n",
    "        return [self.W_f, self.W_i, self.W_C, self.W_o, self.W_v,\n",
    "               self.b_f, self.b_i, self.b_C, self.b_o, self.b_v]\n",
    "        \n",
    "parameters = Parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(x, h_prev, C_prev, p = parameters):\n",
    "    assert x.shape == (X_size, 1)\n",
    "    assert h_prev.shape == (H_size, 1)\n",
    "    assert C_prev.shape == (H_size, 1)\n",
    "    \n",
    "    z = np.row_stack((h_prev, x))\n",
    "    f = sigmoid(np.dot(p.W_f.v, z) + p.b_f.v)\n",
    "    i = sigmoid(np.dot(p.W_i.v, z) + p.b_i.v)\n",
    "    C_bar = tanh(np.dot(p.W_C.v, z) + p.b_C.v)\n",
    "\n",
    "    C = f * C_prev + i * C_bar\n",
    "    o = sigmoid(np.dot(p.W_o.v, z) + p.b_o.v)\n",
    "    h = o * tanh(C)\n",
    "\n",
    "    v = np.dot(p.W_v.v, h) + p.b_v.v\n",
    "    y = np.exp(v) / np.sum(np.exp(v)) #softmax\n",
    "\n",
    "    return z, f, i, C_bar, C, o, h, v, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward(target, dh_next, dC_next, C_prev,\n",
    "    z, f, i, C_bar, C, o, h, v, y,\n",
    "             p = parameters):\n",
    "    \n",
    "    assert z.shape == (X_size + H_size, 1)\n",
    "    assert v.shape == (X_size, 1)\n",
    "    assert y.shape == (X_size, 1)\n",
    "    \n",
    "    for param in [dh_next, dC_next, C_prev, f, i, C_bar, C, o, h]:\n",
    "        assert param.shape == (H_size, 1)\n",
    "        \n",
    "    dv = np.copy(y)\n",
    "    dv[target] -= 1\n",
    "\n",
    "    p.W_v.d += np.dot(dv, h.T)\n",
    "    p.b_v.d += dv\n",
    "\n",
    "    dh = np.dot(p.W_v.v.T, dv)        \n",
    "    dh += dh_next\n",
    "    do = dh * tanh(C)\n",
    "    do = dsigmoid(o) * do\n",
    "    p.W_o.d += np.dot(do, z.T)\n",
    "    p.b_o.d += do\n",
    "\n",
    "    dC = np.copy(dC_next)\n",
    "    dC += dh * o * dtanh(tanh(C))\n",
    "    dC_bar = dC * i\n",
    "    dC_bar = dtanh(C_bar) * dC_bar\n",
    "    p.W_C.d += np.dot(dC_bar, z.T)\n",
    "    p.b_C.d += dC_bar\n",
    "\n",
    "    di = dC * C_bar\n",
    "    di = dsigmoid(i) * di\n",
    "    p.W_i.d += np.dot(di, z.T)\n",
    "    p.b_i.d += di\n",
    "\n",
    "    df = dC * C_prev\n",
    "    df = dsigmoid(f) * df\n",
    "    p.W_f.d += np.dot(df, z.T)\n",
    "    p.b_f.d += df\n",
    "\n",
    "    dz = (np.dot(p.W_f.v.T, df)\n",
    "         + np.dot(p.W_i.v.T, di)\n",
    "         + np.dot(p.W_C.v.T, dC_bar)\n",
    "         + np.dot(p.W_o.v.T, do))\n",
    "    dh_prev = dz[:H_size, :]\n",
    "    dC_prev = f * dC\n",
    "    \n",
    "    return dh_prev, dC_prev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_gradients(params = parameters):\n",
    "    for p in params.all():\n",
    "        p.d.fill(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clip_gradients(params = parameters):\n",
    "    for p in params.all():\n",
    "        np.clip(p.d, -1, 1, out=p.d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_backward(inputs, targets, h_prev, C_prev):\n",
    "    global paramters\n",
    "    \n",
    "    # To store the values for each time step\n",
    "    x_s, z_s, f_s, i_s,  = {}, {}, {}, {}\n",
    "    C_bar_s, C_s, o_s, h_s = {}, {}, {}, {}\n",
    "    v_s, y_s =  {}, {}\n",
    "    \n",
    "    # Values at t - 1\n",
    "    h_s[-1] = np.copy(h_prev)\n",
    "    C_s[-1] = np.copy(C_prev)\n",
    "    \n",
    "    loss = 0\n",
    "    # Loop through time steps\n",
    "    assert len(inputs) == T_steps\n",
    "    for t in range(len(inputs)):\n",
    "        x_s[t] = np.zeros((X_size, 1))\n",
    "        x_s[t][inputs[t]] = 1 # Input character\n",
    "        \n",
    "        (z_s[t], f_s[t], i_s[t],\n",
    "        C_bar_s[t], C_s[t], o_s[t], h_s[t],\n",
    "        v_s[t], y_s[t]) = \\\n",
    "            forward(x_s[t], h_s[t - 1], C_s[t - 1]) # Forward pass\n",
    "            \n",
    "        loss += -np.log(y_s[t][targets[t], 0]) # Loss for at t\n",
    "        \n",
    "    clear_gradients()\n",
    "\n",
    "    dh_next = np.zeros_like(h_s[0]) #dh from the next character\n",
    "    dC_next = np.zeros_like(C_s[0]) #dh from the next character\n",
    "\n",
    "    for t in reversed(range(len(inputs))):\n",
    "        # Backward pass\n",
    "        dh_next, dC_next = \\\n",
    "            backward(target = targets[t], dh_next = dh_next,\n",
    "                     dC_next = dC_next, C_prev = C_s[t-1],\n",
    "                     z = z_s[t], f = f_s[t], i = i_s[t], C_bar = C_bar_s[t],\n",
    "                     C = C_s[t], o = o_s[t], h = h_s[t], v = v_s[t],\n",
    "                     y = y_s[t])\n",
    "\n",
    "    clip_gradients()\n",
    "        \n",
    "    return loss, h_s[len(inputs) - 1], C_s[len(inputs) - 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(h_prev, C_prev, first_char_idx, sentence_length):\n",
    "    x = np.zeros((X_size, 1))\n",
    "    x[first_char_idx] = 1\n",
    "\n",
    "    h = h_prev\n",
    "    C = C_prev\n",
    "\n",
    "    indexes = []\n",
    "    \n",
    "    for t in range(sentence_length):\n",
    "        _, _, _, _, C, _, h, _, p = forward(x, h, C)\n",
    "        idx = np.random.choice(range(X_size), p=p.ravel())\n",
    "        x = np.zeros((X_size, 1))\n",
    "        x[idx] = 1\n",
    "        indexes.append(idx)\n",
    "\n",
    "    return indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_status(inputs, h_prev, C_prev):\n",
    "    #initialized later\n",
    "    global plot_iter, plot_loss\n",
    "    global smooth_loss\n",
    "    \n",
    "    # Get predictions for 200 letters with current model\n",
    "\n",
    "    sample_idx = sample(h_prev, C_prev, inputs[0], 200)\n",
    "    txt = ''.join(idx_to_char[idx] for idx in sample_idx)\n",
    "\n",
    "    # Clear and plot\n",
    "    plt.plot(plot_iter, plot_loss)\n",
    "    display.clear_output(wait=True)\n",
    "    plt.show()\n",
    "\n",
    "    #Print prediction and loss\n",
    "    print(\"----\\n %s \\n----\" % (txt, ))\n",
    "    print(\"iter %d, loss %f\" % (iteration, smooth_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_paramters(params = parameters):\n",
    "    for p in params.all():\n",
    "        p.m += p.d * p.d # Calculate sum of gradients\n",
    "        #print(learning_rate * dparam)\n",
    "        p.v += -(learning_rate * p.d / np.sqrt(p.m + 1e-8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "import signal\n",
    "\n",
    "class DelayedKeyboardInterrupt(object):\n",
    "    def __enter__(self):\n",
    "        self.signal_received = False\n",
    "        self.old_handler = signal.signal(signal.SIGINT, self.handler)\n",
    "\n",
    "    def handler(self, sig, frame):\n",
    "        self.signal_received = (sig, frame)\n",
    "        print('SIGINT received. Delaying KeyboardInterrupt.')\n",
    "\n",
    "    def __exit__(self, type, value, traceback):\n",
    "        signal.signal(signal.SIGINT, self.old_handler)\n",
    "        if self.signal_received:\n",
    "            self.old_handler(*self.signal_received)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exponential average of loss\n",
    "# Initialize to a error of a random model\n",
    "smooth_loss = -np.log(1.0 / X_size) * T_steps\n",
    "\n",
    "iteration, pointer = 0, 0\n",
    "\n",
    "# For the graph\n",
    "plot_iter = np.zeros((0))\n",
    "plot_loss = np.zeros((0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----\n",
      " maal; on knowaat outse\n",
      "quitt heingh,’s ‘and and\n",
      "a canither”.\n",
      "\n",
      "And as are to timen heating the he muchertabilly in a tmeak an ioghto\n",
      "doven tumping at begusteny agean if leag\n",
      "was bothabed, and the Hoome \n",
      "----\n",
      "iter 9004, loss 44.350002\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "while True:\n",
    "    try:\n",
    "        with DelayedKeyboardInterrupt():\n",
    "            # Reset\n",
    "            if pointer + T_steps >= len(data) or iteration == 0:\n",
    "                g_h_prev = np.zeros((H_size, 1))\n",
    "                g_C_prev = np.zeros((H_size, 1))\n",
    "                pointer = 0\n",
    "\n",
    "\n",
    "            inputs = ([char_to_idx[ch] for ch in data[pointer: pointer + T_steps]])\n",
    "            targets = ([char_to_idx[ch] for ch in data[pointer + 1: pointer + T_steps + 1]])\n",
    "\n",
    "            loss, g_h_prev, g_C_prev = \\\n",
    "                forward_backward(inputs, targets, g_h_prev, g_C_prev)\n",
    "            smooth_loss = smooth_loss * 0.999 + loss * 0.001\n",
    "\n",
    "            # Print every hundred steps\n",
    "            if iteration % 100 == 0:\n",
    "                update_status(inputs, g_h_prev, g_C_prev)\n",
    "\n",
    "            update_paramters()\n",
    "\n",
    "            plot_iter = np.append(plot_iter, [iteration])\n",
    "            plot_loss = np.append(plot_loss, [loss])\n",
    "\n",
    "            pointer += T_steps\n",
    "            iteration += 1\n",
    "    except KeyboardInterrupt:\n",
    "        update_status(inputs, g_h_prev, g_C_prev)\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Модель обычно достигает ошибки около 44 после 9000 итераций при тестировании с выборкой из 100 000 символов. \n",
    "    Однако это иногда застревает в локальных минимумах."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4 Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можно проверить, равны ли приблизительные градиенты расчетным аналитическим градиентам (путем backpropagation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import uniform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_numerical_gradient(param, idx, delta, inputs, target, h_prev, C_prev):\n",
    "    old_val = param.v.flat[idx]\n",
    "    \n",
    "    # evaluate loss at [x + delta] and [x - delta]\n",
    "    param.v.flat[idx] = old_val + delta\n",
    "    loss_plus_delta, _, _ = forward_backward(inputs, targets,\n",
    "                                             h_prev, C_prev)\n",
    "    param.v.flat[idx] = old_val - delta\n",
    "    loss_mins_delta, _, _ = forward_backward(inputs, targets, \n",
    "                                             h_prev, C_prev)\n",
    "    \n",
    "    param.v.flat[idx] = old_val #reset\n",
    "\n",
    "    grad_numerical = (loss_plus_delta - loss_mins_delta) / (2 * delta)\n",
    "    # Clip numerical error because analytical gradient is clipped\n",
    "    [grad_numerical] = np.clip([grad_numerical], -1, 1) \n",
    "    \n",
    "    return grad_numerical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_check(num_checks, delta, inputs, target, h_prev, C_prev):\n",
    "    global parameters\n",
    "    \n",
    "    # To calculate computed gradients\n",
    "    _, _, _ =  forward_backward(inputs, targets, h_prev, C_prev)\n",
    "    \n",
    "    \n",
    "    for param in parameters.all():\n",
    "        #Make a copy because this will get modified\n",
    "        d_copy = np.copy(param.d)\n",
    "\n",
    "        # Test num_checks times\n",
    "        for i in range(num_checks):\n",
    "            # Pick a random index\n",
    "            rnd_idx = int(uniform(0, param.v.size))\n",
    "            \n",
    "            grad_numerical = calc_numerical_gradient(param,\n",
    "                                                     rnd_idx,\n",
    "                                                     delta,\n",
    "                                                     inputs,\n",
    "                                                     target,\n",
    "                                                     h_prev, C_prev)\n",
    "            grad_analytical = d_copy.flat[rnd_idx]\n",
    "\n",
    "            err_sum = abs(grad_numerical + grad_analytical) + 1e-09\n",
    "            rel_error = abs(grad_analytical - grad_numerical) / err_sum\n",
    "            \n",
    "            # If relative error is greater than 1e-06\n",
    "            if rel_error > 1e-06:\n",
    "                print('%s (%e, %e) => %e'\n",
    "                      % (param.name, grad_numerical, grad_analytical, rel_error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W_f (6.958984e-05, 6.958928e-05) => 4.062088e-06\n",
      "W_f (6.828316e-07, 6.827892e-07) => 3.102695e-05\n",
      "W_f (-8.668621e-08, -8.622852e-08) => 2.631697e-03\n",
      "W_i (1.029541e-05, 1.029518e-05) => 1.104474e-05\n",
      "W_i (4.921432e-05, 4.921377e-05) => 5.562066e-06\n",
      "W_C (1.601755e-04, 1.601747e-04) => 2.441997e-06\n",
      "W_v (8.407142e-06, 8.407344e-06) => 1.202627e-05\n",
      "W_v (3.378631e-07, 3.381434e-07) => 4.140422e-04\n",
      "W_v (-1.154632e-06, -1.154463e-06) => 7.301813e-05\n",
      "b_f (-4.919702e-03, -4.919642e-03) => 6.050972e-06\n",
      "b_f (-2.004849e-02, -2.004842e-02) => 1.609175e-06\n",
      "b_f (-2.004849e-02, -2.004842e-02) => 1.609175e-06\n",
      "b_f (6.435485e-02, 6.435280e-02) => 1.596124e-05\n",
      "b_i (-6.419242e-02, -6.419222e-02) => 1.540923e-06\n",
      "b_i (-8.623683e-02, -8.623656e-02) => 1.542887e-06\n",
      "b_i (-2.885017e-02, -2.885011e-02) => 1.055722e-06\n",
      "b_i (-2.882515e-02, -2.882525e-02) => 1.676248e-06\n",
      "b_i (1.994857e-02, 1.994862e-02) => 1.087268e-06\n",
      "b_i (1.948465e-02, 1.948470e-02) => 1.289304e-06\n",
      "b_o (-1.599535e-01, -1.599555e-01) => 6.214903e-06\n",
      "b_o (1.023817e-01, 1.023803e-01) => 6.674832e-06\n",
      "b_o (-6.773583e-02, -6.773472e-02) => 8.240174e-06\n",
      "b_o (-6.582505e-02, -6.582363e-02) => 1.078592e-05\n",
      "b_o (2.443018e-02, 2.442913e-02) => 2.143009e-05\n",
      "b_o (-9.147519e-02, -9.147421e-02) => 5.386097e-06\n",
      "b_o (-9.499052e-03, -9.498785e-03) => 1.409129e-05\n",
      "b_o (8.784551e-03, 8.784327e-03) => 1.271179e-05\n"
     ]
    }
   ],
   "source": [
    "gradient_check(10, 1e-5, inputs, targets, g_h_prev, g_C_prev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Способы борьбы с исчезающим градиентом в RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Нашел теорию тут,\n",
    "\n",
    "https://medium.com/datadriveninvestor/how-do-lstm-networks-solve-the-problem-of-vanishing-gradients-a6784971a577\n",
    "\n",
    "Долгосрочные зависимости и отношения кодируются в векторах состояния ячейки, и именно производная состояния ячейки может предотвратить исчезновение градиентов LSTM. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
